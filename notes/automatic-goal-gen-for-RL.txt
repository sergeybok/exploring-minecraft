Main idea: goal parametrized reward functions generated by the network
    instead of having one reward function for the ultimate objective
    you have the network generate its own goals which as I understood
    are the stepping stones towards the ultimate objective (but as I
    understand it these stepping-stone goals are not guaranteed to
    actually direct you towards the main objective). These goals are
    generated with a Goal GAN for which the pseudocode is in the paper
    and they explain it extensively but I am not sure that I like this
    conception.


Takeaway ideas:
    continueous goal spaces, that way we can caluclate the distance from
    the the current state to the goal state. but for this you need a function
    which maps froms state-space to goal-space.

    And generating goals that are needed to be done to achieve the ultimate
    objective but not sure if their approach is best or we can come up wtih
    something better.














